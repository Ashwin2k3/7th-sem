Table of Contents
=================

   * [<button style="--button-color:fg.muted" type="button" aria-label="Expand file tree" data-testid="expand-file-tree-button-mobile" class="types__StyledButton-sc-ws60qy-0 kBQCgV" data-loading="false" data-size="medium" aria-describedby=":Rld9lab:-loading-announcement"><span data-component="buttonContent" class="Box-sc-g0xbh4-0 gUkoLg"><span data-component="leadingVisual" class="Box-sc-g0xbh4-0 hzSPyu"><svg aria-hidden="true" focusable="false" class="octicon octicon-arrow-left" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M7.78 12.53a.75.75 0 0 1-1.06 0L2.47 8.28a.75.75 0 0 1 0-1.06l4.25-4.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L4.81 7h7.44a.75.75 0 0 1 0 1.5H4.81l2.97 2.97a.75.75 0 0 1 0 1.06Z"></path></svg></span><span data-component="text">Files</span></span></button><span role="tooltip" aria-label="Expand file tree" id="expand-button-file-tree-button" class="Tooltip__TooltipBase-sc-17tf59c-0 hWlpPn tooltipped-se"><button data-component="IconButton" type="button" data-testid="expand-file-tree-button" aria-controls="repos-file-tree" class="types__StyledButton-sc-ws60qy-0 LXqWf prc-Button-IconButton-szpyj" data-loading="false" data-no-visuals="true" data-size="medium" aria-describedby=":R35d9lab:-loading-announcement" aria-labelledby="expand-button-file-tree-button"><svg aria-hidden="true" focusable="false" class="octicon octicon-sidebar-collapse" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M6.823 7.823a.25.25 0 0 1 0 .354l-2.396 2.396A.25.25 0 0 1 4 10.396V5.604a.25.25 0 0 1 .427-.177Z"></path><path d="M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25H9.5v-13H1.75a.25.25 0 0 0-.25.25ZM11 14.5h3.25a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H11Z"></path></svg></button></span><button hidden="" data-testid="" data-hotkey-scope="read-only-cursor-text-area"></button></h2></div><div class="react-code-view-header-mb--narrow mr-2"><button type="button" aria-haspopup="true" aria-expanded="false" tabindex="0" aria-label="main branch" data-testid="anchor-button" class="types__StyledButton-sc-ws60qy-0 iQGjqs ref-selector-class" data-loading="false" data-size="medium" aria-describedby="branch-picker-repos-header-ref-selector-wide-loading-announcement" id="branch-picker-repos-header-ref-selector-wide"><span data-component="buttonContent" class="Box-sc-g0xbh4-0 gUkoLg"><span data-component="text"><div class="Box-sc-g0xbh4-0 bZBlpz"><div class="Box-sc-g0xbh4-0 lhTYNA"><svg aria-hidden="true" focusable="false" class="octicon octicon-git-branch" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z"></path></svg></div><div class="Box-sc-g0xbh4-0 dbrgmi ref-selector-button-text-container"><span class="Text__StyledText-sc-17v1xeu-0 eMMFM"> <!-- -->main</span></div></div></span><span data-component="trailingVisual" class="Box-sc-g0xbh4-0 hzSPyu"><svg aria-hidden="true" focusable="false" class="octicon octicon-triangle-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z"></path></svg></span></span></button><button hidden="" data-hotkey-scope="read-only-cursor-text-area"></button></div><div class="Box-sc-g0xbh4-0 dHJiml react-code-view-header-mb--narrow"><div class="Box-sc-g0xbh4-0 cEytCf"><nav data-testid="breadcrumbs" aria-labelledby="repos-header-breadcrumb-heading" id="repos-header-breadcrumb" class="Box-sc-g0xbh4-0 fzFXnm"><h2 class="sr-only prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading" id="repos-header-breadcrumb-heading">Breadcrumbs</h2><ol class="Box-sc-g0xbh4-0 iMnkmv"><li class="Box-sc-g0xbh4-0 ghzDag"><a class="Link__StyledLink-sc-14289xe-0 iuEHTz" sx="[object Object]" data-testid="breadcrumbs-repo-link" href="/Ashwin2k3/7th-sem/tree/main">7th-sem</a></li></ol></nav><div data-testid="breadcrumbs-filename" class="Box-sc-g0xbh4-0 ghzDag"><span class="Text__StyledText-sc-17v1xeu-0 HlHVj" aria-hidden="true">/</span><h1 class="Box-sc-g0xbh4-0 jGhzSQ prc-Heading-Heading-6CmGO" tabindex="-1" id="file-name-id">Reinforcement_learning_and_Deep_learning</h1><span class="Text__StyledText-sc-17v1xeu-0 HlHVj" aria-hidden="true">/</span></div><div aria-describedby=":R1td9lab:"><button data-component="IconButton" type="button" aria-label="Copy path" tabindex="0" class="types__StyledButton-sc-ws60qy-0 dPmZyJ prc-Button-IconButton-szpyj" data-loading="false" data-no-visuals="true" data-size="small" aria-describedby=":R7td9lab:-loading-announcement"><svg aria-hidden="true" focusable="false" class="octicon octicon-copy" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path></svg></button></div></div></div></div><div class="react-code-view-header-element--wide"><div class="Box-sc-g0xbh4-0 faNtbn"><div class="d-flex gap-2"><h2 class="sr-only prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading">Directory actions</h2><div><div class="Box-sc-g0xbh4-0 fmQaBv"><span class="TextInputWrapper__TextInputBaseWrapper-sc-1mqhpbi-0 TextInputWrapper-sc-1mqhpbi-1 gwqFqs decvaq TextInput-wrapper" aria-busy="false"><span class="TextInput-icon"><svg aria-hidden="true" focusable="false" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path></svg></span><input type="text" aria-label="Go to file" role="combobox" aria-controls="file-results-list" aria-expanded="false" aria-haspopup="dialog" autoCorrect="off" spellcheck="false" placeholder="Go to file" data-component="input" class="UnstyledTextInput-sc-14ypya-0 kbCLEG" value=""/><span class="TextInput-icon"></span></span></div><button hidden="" data-testid="" data-hotkey-scope="read-only-cursor-text-area"></button><button hidden=""></button></div><h2 class="sr-only prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading">More options</h2><button data-component="IconButton" type="button" aria-label="More options" title="More options" data-testid="tree-overflow-menu-anchor" aria-haspopup="true" aria-expanded="false" tabindex="0" class="types__StyledButton-sc-ws60qy-0 JVsBq prc-Button-IconButton-szpyj" data-loading="false" data-no-visuals="true" data-size="medium" aria-describedby=":Rlqd9lab:-loading-announcement" id=":Rlqd9lab:"><svg aria-hidden="true" focusable="false" class="octicon octicon-kebab-horizontal" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path></svg></button><a class="Link__StyledLink-sc-14289xe-0 dIlPa js-github-dev-shortcut d-none"></a><a class="Link__StyledLink-sc-14289xe-0 dIlPa js-github-dev-new-tab-shortcut d-none" target="_blank"></a></div></div></div><div class="react-code-view-header-element--narrow"><div class="Box-sc-g0xbh4-0 faNtbn"><div class="d-flex gap-2"><h2 class="sr-only prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading">Directory actions</h2><h2 class="sr-only prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading">More options</h2><button data-component="IconButton" type="button" aria-label="More options" title="More options" data-testid="tree-overflow-menu-anchor" aria-haspopup="true" aria-expanded="false" tabindex="0" class="types__StyledButton-sc-ws60qy-0 JVsBq prc-Button-IconButton-szpyj" data-loading="false" data-no-visuals="true" data-size="medium" aria-describedby=":Rlrd9lab:-loading-announcement" id=":Rlrd9lab:"><svg aria-hidden="true" focusable="false" class="octicon octicon-kebab-horizontal" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path></svg></button><a class="Link__StyledLink-sc-14289xe-0 dIlPa js-github-dev-shortcut d-none"></a><a class="Link__StyledLink-sc-14289xe-0 dIlPa js-github-dev-new-tab-shortcut d-none" target="_blank"></a></div></div></div></div></div></div></div></div><div class="Box-sc-g0xbh4-0 dJxjrT react-code-view-bottom-padding"></div><div class="Box-sc-g0xbh4-0 dJxjrT"><div class="d-flex flex-column border rounded-2 mb-3 pl-1"><div class="Box-sc-g0xbh4-0 dzCJzi"><h2 class="sr-only prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading">Latest commit</h2><div style="width:120px" class="Skeleton Skeleton--text" data-testid="loading"> </div><div class="d-flex flex-shrink-0 gap-2"><div data-testid="latest-commit-details" class="d-none d-sm-flex flex-items-center"></div><div class="d-flex gap-2"><h2 class="sr-only prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading">History</h2><a href="/Ashwin2k3/7th-sem/commits/main/Reinforcement_learning_and_Deep_learning" class="types__StyledButton-sc-ws60qy-0 dPmZyJ d-none d-lg-flex LinkButton-module__code-view-link-button--xvCGA flex-items-center fgColor-default" data-loading="false" data-size="small" aria-describedby=":R5d6l9lab:-loading-announcement"><span data-component="buttonContent" class="Box-sc-g0xbh4-0 gUkoLg"><span data-component="leadingVisual" class="Box-sc-g0xbh4-0 hzSPyu"><svg aria-hidden="true" focusable="false" class="octicon octicon-history" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z"></path></svg></span><span data-component="text"><span class="fgColor-default">History</span></span></span></a><div class="d-sm-none"></div><div class="d-flex d-lg-none"><span role="tooltip" aria-label="History" id="history-icon-button-tooltip" class="Tooltip__TooltipBase-sc-17tf59c-0 hWlpPn tooltipped-n"><a href="/Ashwin2k3/7th-sem/commits/main/Reinforcement_learning_and_Deep_learning" class="types__StyledButton-sc-ws60qy-0 dPmZyJ LinkButton-module__code-view-link-button--xvCGA flex-items-center fgColor-default" data-loading="false" data-size="small" aria-describedby=":Rpd6l9lab:-loading-announcement history-icon-button-tooltip"><span data-component="buttonContent" class="Box-sc-g0xbh4-0 gUkoLg"><span data-component="leadingVisual" class="Box-sc-g0xbh4-0 hzSPyu"><svg aria-hidden="true" focusable="false" class="octicon octicon-history" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z"></path></svg></span></span></a></span></div></div></div></div></div><div class="Box-sc-g0xbh4-0 bEZNi react-blob-view-header-sticky"><div class="Box-sc-g0xbh4-0 gjQlJX"><div class="Box-sc-g0xbh4-0 hqwSEx"><div class="Box-sc-g0xbh4-0 bDVoEr"><div class="Box-sc-g0xbh4-0 kYLlPM"><div class="Box-sc-g0xbh4-0 gYjEmn"><button type="button" aria-haspopup="true" aria-expanded="false" tabindex="0" aria-label="main branch" data-testid="anchor-button" class="types__StyledButton-sc-ws60qy-0 iQGjqs ref-selector-class" data-loading="false" data-size="medium" aria-describedby="branch-picker-repos-header-ref-selector-loading-announcement" id="branch-picker-repos-header-ref-selector"><span data-component="buttonContent" class="Box-sc-g0xbh4-0 gUkoLg"><span data-component="text"><div class="Box-sc-g0xbh4-0 bZBlpz"><div class="Box-sc-g0xbh4-0 lhTYNA"><svg aria-hidden="true" focusable="false" class="octicon octicon-git-branch" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M9.5 3.25a2.25 2.25 0 1 1 3 2.122V6A2.5 2.5 0 0 1 10 8.5H6a1 1 0 0 0-1 1v1.128a2.251 2.251 0 1 1-1.5 0V5.372a2.25 2.25 0 1 1 1.5 0v1.836A2.493 2.493 0 0 1 6 7h4a1 1 0 0 0 1-1v-.628A2.25 2.25 0 0 1 9.5 3.25Zm-6 0a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Zm8.25-.75a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5ZM4.25 12a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Z"></path></svg></div><div class="Box-sc-g0xbh4-0 dbrgmi ref-selector-button-text-container"><span class="Text__StyledText-sc-17v1xeu-0 eMMFM"> <!-- -->main</span></div></div></span><span data-component="trailingVisual" class="Box-sc-g0xbh4-0 hzSPyu"><svg aria-hidden="true" focusable="false" class="octicon octicon-triangle-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="m4.427 7.427 3.396 3.396a.25.25 0 0 0 .354 0l3.396-3.396A.25.25 0 0 0 11.396 7H4.604a.25.25 0 0 0-.177.427Z"></path></svg></span></span></button><button hidden="" data-hotkey-scope="read-only-cursor-text-area"></button></div><div class="Box-sc-g0xbh4-0 kGqOLL"><div class="Box-sc-g0xbh4-0 fHind"><nav data-testid="breadcrumbs" aria-labelledby="sticky-breadcrumb-heading" id="sticky-breadcrumb" class="Box-sc-g0xbh4-0 fzFXnm"><h2 class="sr-only prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading" id="sticky-breadcrumb-heading">Breadcrumbs</h2><ol class="Box-sc-g0xbh4-0 iMnkmv"><li class="Box-sc-g0xbh4-0 ghzDag"><a class="Link__StyledLink-sc-14289xe-0 iuEHTz" sx="[object Object]" data-testid="breadcrumbs-repo-link" href="/Ashwin2k3/7th-sem/tree/main">7th-sem</a></li></ol></nav><div data-testid="breadcrumbs-filename" class="Box-sc-g0xbh4-0 ghzDag"><span class="Text__StyledText-sc-17v1xeu-0 lauzFl" aria-hidden="true">/</span><h1 class="Box-sc-g0xbh4-0 dnZoUW prc-Heading-Heading-6CmGO" tabindex="-1" id="sticky-file-name-id">Reinforcement_learning_and_Deep_learning</h1><span class="Text__StyledText-sc-17v1xeu-0 HlHVj" aria-hidden="true">/</span></div></div></div></div><button style="--button-color:fg.default" type="button" class="types__StyledButton-sc-ws60qy-0 fwExmK" data-loading="false" data-size="small" aria-describedby=":R2el9lab:-loading-announcement"><span data-component="buttonContent" class="Box-sc-g0xbh4-0 gUkoLg"><span data-component="leadingVisual" class="Box-sc-g0xbh4-0 hzSPyu"><svg aria-hidden="true" focusable="false" class="octicon octicon-arrow-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M3.47 7.78a.75.75 0 0 1 0-1.06l4.25-4.25a.75.75 0 0 1 1.06 0l4.25 4.25a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018L9 4.81v7.44a.75.75 0 0 1-1.5 0V4.81L4.53 7.78a.75.75 0 0 1-1.06 0Z"></path></svg></span><span data-component="text">Top</span></span></button></div></div></div></div><div class="Box-sc-g0xbh4-0 vIPPs"><div data-hpc="true"><button hidden="" data-testid="focus-next-element-button" data-hotkey="j"></button><button hidden="" data-testid="focus-previous-element-button" data-hotkey="k"></button><h2 class="sr-only prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading" id="folders-and-files">Folders and files</h2><table aria-labelledby="folders-and-files" class="Box-sc-g0xbh4-0 fdROMU"><thead class="Box-sc-g0xbh4-0 eHDvEW"><tr class="Box-sc-g0xbh4-0 jdgHnn"><th colSpan="2" class="Box-sc-g0xbh4-0 bQivRW"><span class="text-bold">Name</span></th><th colSpan="1" class="Box-sc-g0xbh4-0 ldkMIO"><span class="text-bold">Name</span></th><th class="hide-sm"><div title="Last commit message" class="Truncate__StyledTruncate-sc-23o1d2-0 liVpTx width-fit"><span class="text-bold">Last commit message</span></div></th><th colSpan="1" class="Box-sc-g0xbh4-0 jMbWeI"><div title="Last commit date" class="Truncate__StyledTruncate-sc-23o1d2-0 liVpTx width-fit"><span class="text-bold">Last commit date</span></div></th></tr></thead><tbody><tr id="folder-row-0" class="Box-sc-g0xbh4-0 cgFZpq"><td colSpan="3" class="f5 text-normal px-3"><h3 class="sr-only prc-Heading-Heading-6CmGO" data-testid="screen-reader-heading">parent directory</h3><a class="Link__StyledLink-sc-14289xe-0 iuTGUw" aria-label="Parent directory" data-react-autofocus="true" data-testid="up-tree" muted="" rel="nofollow" sx="[object Object]" href="/Ashwin2k3/7th-sem/tree/main"><div class="Box-sc-g0xbh4-0 hHzTZW width-full"><svg aria-hidden="true" focusable="false" class="Octicon-sc-9kayk9-0 jdVJbO" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M1.75 1A1.75 1.75 0 0 0 0 2.75v10.5C0 14.216.784 15 1.75 15h12.5A1.75 1.75 0 0 0 16 13.25v-8.5A1.75 1.75 0 0 0 14.25 3H7.5a.25.25 0 0 1-.2-.1l-.9-1.2C6.07 1.26 5.55 1 5 1H1.75Z"></path></svg>..</div></a></td></tr><tr class="react-directory-row undefined" id="folder-row-1"><td class="react-directory-row-name-cell-small-screen" colSpan="2"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="README.md" aria-label="README.md, (File)" class="Link--primary" href="/Ashwin2k3/7th-sem/blob/main/Reinforcement_learning_and_Deep_learning/README.md">README.md</a></div></div></div></div></td><td class="react-directory-row-name-cell-large-screen" colSpan="1"><div class="react-directory-filename-column"><svg aria-hidden="true" focusable="false" class="color-fg-muted" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M2 1.75C2 .784 2.784 0 3.75 0h6.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v9.586A1.75 1.75 0 0 1 13.25 16h-9.5A1.75 1.75 0 0 1 2 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h9.5a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 9 4.25V1.5Zm6.75.062V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path></svg><div class="overflow-hidden"><div class="react-directory-filename-cell"><div class="react-directory-truncate"><a title="README.md" aria-label="README.md, (File)" class="Link--primary" href="/Ashwin2k3/7th-sem/blob/main/Reinforcement_learning_and_Deep_learning/README.md">README.md</a></div></div></div></div></td><td class="react-directory-row-commit-cell"><div class="Skeleton Skeleton--text"> </div></td><td><div class="Skeleton Skeleton--text"> </div></td></tr><tr class="Box-sc-g0xbh4-0 eNCcrz d-none" data-testid="view-all-files-row"><td colSpan="3" class="Box-sc-g0xbh4-0 bHTcCe"><div><button class="Link__StyledLink-sc-14289xe-0 dIlPa">View all files</button></div></td></tr></tbody></table></div><div class="Box-sc-g0xbh4-0 kkSYPE"><div id="readme" class="Box-sc-g0xbh4-0 dYOCLB"><div class="Box-sc-g0xbh4-0 gGRoah"><h2 class="Box-sc-g0xbh4-0 caoWpU prc-Heading-Heading-6CmGO"><a class="Link__StyledLink-sc-14289xe-0 gsznkV" href="#readme">README.md</a></h2><button data-component="IconButton" type="button" aria-label="Outline" aria-pressed="false" class="types__StyledButton-sc-ws60qy-0 gAIXvS prc-Button-IconButton-szpyj" data-loading="false" data-no-visuals="true" data-size="small" aria-describedby=":R6mil9lab:-loading-announcement"><svg aria-hidden="true" focusable="false" class="octicon octicon-list-unordered" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg></button></div><div class="Box-sc-g0xbh4-0 QkQOb js-snippet-clipboard-copy-unpositioned" data-hpc="true"><article class="markdown-body entry-content container-lg" itemprop="text"><div class="markdown-heading" dir="auto"><h1 tabindex="-1" class="heading-element" dir="auto"><strong>UNIT - I</strong>](/Ashwin2k3/7th-sem/tree/main)
      * [Introduction to Reinforcement Learning and Its Terms](#introduction-to-reinforcement-learning-and-its-terms)
      * [<strong>1. Features and Elements of Reinforcement Learning (RL):</strong>](#1-features-and-elements-of-reinforcement-learning-rl)
      * [<strong>2. Defining RL Framework and Markov Decision Process (MDP):</strong>](#2-defining-rl-framework-and-markov-decision-process-mdp)
      * [<strong>3. Policies:</strong>](#3-policies)
      * [<strong>4. Value Functions and Bellman Equations:</strong>](#4-value-functions-and-bellman-equations)
      * [<strong>4. Exploration vs. Exploitation:</strong>](#4-exploration-vs-exploitation)
      * [1. Code Standards and Libraries Used in RL (Python/Keras/TensorFlow), Tabular Methods, and Q-networks:](#1-code-standards-and-libraries-used-in-rl-pythonkerastensorflow-tabular-methods-and-q-networks)
      * [2. Planning through the use of Dynamic Programming and Monte Carlo:](#2-planning-through-the-use-of-dynamic-programming-and-monte-carlo)
      * [3. Temporal-Difference (TD) Learning Methods (TD(0), SARSA, Q-Learning):](#3-temporal-difference-td-learning-methods-td0-sarsa-q-learning)
      * [4. Deep Q-networks (DQN, DDQN, Dueling DQN, Prioritized Experience Replay):](#4-deep-q-networks-dqn-ddqn-dueling-dqn-prioritized-experience-replay)
* [<strong>UNIT - 2</strong>](#unit---2)
      * [1. Policy Optimization: Introduction to Policy-Based Methods](#1-policy-optimization-introduction-to-policy-based-methods)
      * [2. Vanilla Policy Gradient](#2-vanilla-policy-gradient)
      * [3. REINFORCE Algorithm and Stochastic Policy Search](#3-reinforce-algorithm-and-stochastic-policy-search)
      * [4. Actor-Critic Methods (A2C, A3C)](#4-actor-critic-methods-a2c-a3c)
      * [5. Advanced Policy Gradient (PPO, TRPO, DDPG)](#5-advanced-policy-gradient-ppo-trpo-ddpg)
      * [6. Model-Based RL](#6-model-based-rl)
      * [7. Recent Advances and Applications: Meta-Learning, Multi-Agent Reinforcement Learning](#7-recent-advances-and-applications-meta-learning-multi-agent-reinforcement-learning)
      * [8. Partially Observable Markov Decision Process (POMDP)](#8-partially-observable-markov-decision-process-pomdp)
      * [9. Applying RL for Real-World Problems](#9-applying-rl-for-real-world-problems)








# **UNIT - I**
### Introduction to Reinforcement Learning and Its Terms

1. **Reinforcement Learning (RL)**: RL is a type of machine learning where agents learn to make decisions by interacting with an environment to maximize cumulative rewards.
2. **Agent**: The learner or decision-maker in RL that interacts with the environment by taking actions.
3. **Environment**: The system or world the agent interacts with, providing feedback in the form of rewards based on the agent’s actions.
4. **State**: A representation of the environment’s current situation as observed by the agent, which may be fully or partially observable.
5. **Action**: The decision or move the agent makes at any given time step, which affects the environment and leads to new states.
6. **Reward**: A numerical value provided by the environment as feedback after each action, indicating the immediate benefit of the agent's action.
7. **Policy (π)**: A strategy that defines how the agent selects actions based on states, either deterministically or stochastically.
8. **Value Function**: A function that estimates the expected cumulative reward (or value) of being in a particular state or taking a specific action.
9. **Episode**: A sequence of interactions between the agent and the environment that starts from an initial state and ends in a terminal state or predefined condition.
10. **Discount Factor (γ)**: A parameter that determines the importance of future rewards, with values closer to 1 making future rewards more significant to the agent's decision-making process.

Here's a breakdown of each topic related to Reinforcement Learning (RL), with 10 points under each heading:

### **1. Features and Elements of Reinforcement Learning (RL):**
1. **Learning from interaction:** RL agents learn by interacting with the environment.
2. **Reward signal:** Agents receive a reward or punishment after every action to understand how good or bad the action was.
3. **Agent:** The entity that takes actions in the environment.
4. **Environment:** The world through which the agent moves and interacts.
5. **State:** The current situation or observation the agent perceives from the environment.
6. **Action:** The choices or moves made by the agent at each step.
7. **Policy:** The strategy or mapping from states to actions that the agent follows.
8. **Value function:** Estimates how good it is for the agent to be in a given state or how valuable a particular state is.
9. **Exploration vs. Exploitation:** The trade-off between exploring new actions to learn more and exploiting current knowledge to maximize reward.
10. **Delayed rewards:** Actions may have long-term consequences, meaning the reward could be delayed over time.

---

### **2. Defining RL Framework and Markov Decision Process (MDP):**
1. **MDP formulation:** RL problems are framed as Markov Decision Processes (MDPs).
2. **MDP Components:** MDP consists of states, actions, rewards, and transition probabilities.
3. **Markov Property:** The future state depends only on the current state and action, not the past states (memoryless property).
4. **State transition function:** Specifies the probability of moving from one state to another given a certain action.
5. **Reward function:** Maps each action and state transition to a numerical reward.
6. **Discount factor (γ):** Determines the importance of future rewards (values between 0 and 1).
7. **Horizon:** Time horizon refers to whether the agent is dealing with finite or infinite steps.
8. **Objective:** The agent’s goal is to maximize the cumulative (discounted) reward over time.
9. **Optimal policy in MDP:** The policy that maximizes the expected return (reward) from every state.
10. **Bellman equations:** Used to compute the value functions in MDP, setting the foundation for solving RL problems.

---

### **3. Policies:**
1. **Policy definition:** A policy is a mapping from states to actions, guiding the agent’s behavior.
2. **Deterministic policy:** A policy where each state maps to a single action (e.g., π(s) = a).
3. **Stochastic policy:** A policy where each state maps to a probability distribution over actions (π(a|s)).
4. **Optimal policy:** The policy that maximizes the expected sum of rewards.
5. **Policy iteration:** An algorithm to find the optimal policy by improving policies iteratively.
6. **On-policy learning:** The agent learns the policy that it is currently using.
7. **Off-policy learning:** The agent learns a policy different from the one it is currently following.
8. **Exploration policy:** A policy that favors actions that have not been tried much (e.g., ε-greedy).
9. **Greedy policy:** A policy that always selects the action with the highest expected reward.
10. **Policy gradient methods:** Algorithms that optimize the policy directly by following the gradient of expected reward.

---

### **4. Value Functions and Bellman Equations:**
1. **Value function (V):** Predicts the total reward an agent can expect to collect starting from a given state.
2. **State-value function V(s):** The expected return starting from state `s` and following policy `π`.
3. **Action-value function Q(s, a):** The expected return starting from state `s`, taking action `a`, and following policy `π`.
4. **Optimal value function (V*):** The maximum value function achievable by any policy from a given state.
5. **Optimal action-value function (Q*):** The maximum expected return after taking an action and following the optimal policy.
6. **Bellman equation for V:** Breaks down the value of a state into immediate reward plus the discounted value of the next state.
7. **Bellman equation for Q:** Breaks down Q-value into immediate reward and maximum future Q-value.
8. **Bellman Optimality Equation:** Provides recursive relationships to find the optimal policy using dynamic programming.
9. **Temporal Difference (TD) learning:** Updates value functions based on differences between predicted and observed rewards.
10. **Value iteration:** An algorithm to compute optimal policies by iterating on the Bellman equations.




---

### **4. Exploration vs. Exploitation:**

Here's a table that shows the differences between **Exploration** and **Exploitation**:

| **Aspect**                          | **Exploration**                                | **Exploitation**                               |
|-------------------------------------|------------------------------------------------|------------------------------------------------|
| **Definition**                      | Trying new actions to gather more information. | Using known actions to maximize immediate rewards. |
| **Goal**                            | Discover new, potentially better actions.      | Maximize the reward based on existing knowledge. |
| **Action Choice**                   | Chooses actions that haven't been tried much.  | Chooses the action with the highest known reward. |
| **Risk**                            | Higher risk, as the outcome is uncertain.      | Lower risk, as it relies on past knowledge.      |
| **Focus**                           | Long-term benefit by learning more about the environment. | Short-term benefit by maximizing the current reward. |
| **Common Strategy**                 | ε-greedy, Boltzmann exploration, UCB, Thompson Sampling. | Greedy or deterministic policy (choosing the best known action). |
| **Frequency of Use**                | Often used early in learning when little is known. | Used more frequently as the agent gains knowledge about the environment. |
| **Effect on Knowledge**             | Helps the agent discover new strategies or rewards. | Reinforces the agent’s current understanding of the best strategy. |
| **Outcome**                         | Can lead to higher long-term rewards if better actions are found. | Ensures consistent, reliable rewards based on current knowledge. |
| **Usage in Unknown Environments**   | Crucial in environments where the agent has little information. | Effective when the environment is well-understood by the agent. |

This table highlights the key differences between **exploration** and **exploitation** in reinforcement learning.

Here are the points with bold keywords before each line:

### 1. Code Standards and Libraries Used in RL (Python/Keras/TensorFlow), Tabular Methods, and Q-networks:

**Code Standards:**
- **PEP8:** Follow PEP8 style guidelines.
- **Naming:** Use clear function and variable names to enhance readability.
- **Modularity:** Modularize code for better maintainability and reusability.
- **Testing:** Write unit tests for critical functions.
- **Version Control:** Use version control (e.g., Git) for collaborative development.
- **Logging:** Use logging for tracking and debugging.
- **Environment Management:** Use environment management tools like `virtualenv` or `conda`.
- **Commenting:** Comment code clearly, especially in complex algorithmic implementations.
- **Documentation:** Use docstrings to explain class and method functionality.
- **Optimization:** Optimize performance when handling large neural network models.

**Libraries (Python/Keras/TensorFlow):**
- **Python:** Main language for implementing RL algorithms and frameworks.
- **Keras:** High-level API for building and training neural networks in RL tasks.
- **TensorFlow:** Backend for Keras, enables efficient GPU computations.
- **OpenAI Gym:** Provides environments for testing RL algorithms.
- **NumPy:** Used for matrix and vector operations in tabular RL methods.
- **Pandas:** Useful for data manipulation and preprocessing before feeding to models.
- **Matplotlib/Seaborn:** Visualization libraries for plotting rewards and performance.
- **Stable-Baselines3:** A collection of reliable RL algorithms implemented in Python.
- **Pytorch:** Alternative to TensorFlow for RL, often used for research.
- **SciPy/Sklearn:** Useful for statistics and machine learning model integration.

**Tabular Methods:**
- **State-Action Table:** Tabular methods store value functions in tables (state-action pairs).
- **Simplicity:** Used when the state-action space is small and can be explicitly enumerated.
- **Optimal Cases:** Optimal for simple environments like GridWorld or Tic-Tac-Toe.
- **Scaling Issue:** Does not scale well for large or continuous state spaces.
- **Key Algorithms:** Algorithms include Policy Iteration and Value Iteration.
- **Dynamic Programming:** Uses dynamic programming to evaluate and improve policies.
- **Combination with Q-Learning:** Can be combined with Q-learning for tabular Q-values.
- **Inefficiency in High Dimensions:** Inefficient in high-dimensional environments.
- **Learning Reinforcement:** Reinforces explicit learning through action-value functions (Q-values).
- **Theoretical Development:** Used for theoretical development and proof of RL concepts.

**Q-networks:**
- **Neural Approximation:** Neural network approximation of the Q-value function (Q(s, a)).
- **Scalability:** Scales RL to large state-action spaces where tabular methods fail.
- **Gradient Descent:** Trained using gradient descent to minimize loss.
- **Function Approximation:** Helps with function approximation in continuous and high-dimensional spaces.
- **Combination:** Combines deep learning and RL techniques (Deep Q-Networks).
- **Fast Learning:** Enables faster learning in complex environments like Atari games.
- **Output Q-values:** Outputs Q-values for each possible action given a state.
- **Experience Replay:** Uses experience replay to improve sample efficiency.
- **Bellman Equation:** Trains with the Bellman equation to update Q-values.
- **Policy Improvement:** Allows for policy improvement based on learned Q-values.

---

### 2. Planning through the use of Dynamic Programming and Monte Carlo:

**Dynamic Programming (DP):**
- **Full Model Required:** Requires a complete model of the environment (transition and reward functions).
- **Iterative Computation:** Involves iterating over all states to compute value functions.
- **Key Algorithms:** Key algorithms include Policy Iteration and Value Iteration.
- **Efficiency in Small MDPs:** Efficient for small, finite MDPs.
- **Policy Iteration:** Policy Iteration alternates between policy evaluation and policy improvement.
- **Value Iteration:** Value Iteration performs one-step lookahead and iterates to find the optimal value function.
- **Knowledge Requirement:** Used for planning when the agent has full knowledge of the environment.
- **Convergence Guarantee:** Guarantees convergence to the optimal policy.
- **Non-Interactive:** Does not require interaction with the environment, unlike RL.
- **Scalability Issue:** Inefficient for large state spaces or unknown environments.

**Monte Carlo (MC) methods:**
- **Sampling Learning:** Learns value functions through sampling without requiring full knowledge of the environment.
- **Return Averaging:** Based on averaging returns from complete episodes.
- **Episode Termination:** Requires episodes to terminate to compute the full return.
- **Handling Large Spaces:** Handles large or continuous state spaces better than DP.
- **Policy Updates:** Updates policy based on sample episodes instead of exhaustive iterations.
- **Unknown Transition Probabilities:** Works well in environments where the transition probabilities are unknown.
- **Episodic Methods:** Stochastic approximation rather than deterministic updates like in DP.
- **Function Approximation:** Can be used in combination with function approximation methods.
- **Convergence Speed:** Slower convergence compared to dynamic programming in some settings.

---

### 3. Temporal-Difference (TD) Learning Methods (TD(0), SARSA, Q-Learning):

**TD(0):**
- **Hybrid Approach:** A combination of Monte Carlo and dynamic programming methods.
- **Immediate Feedback:** Updates value estimates based on the most recent experience.
- **Sample Efficiency:** Requires fewer samples compared to Monte Carlo methods.
- **Bootstrapping:** Uses bootstrapping, relying on current estimates to update value functions.
- **Single Step Update:** Updates values after each step rather than waiting for an episode to finish.
- **Direct Learning:** Useful in continuous tasks where episodes may not be defined.
- **Generalization:** Can generalize across similar states using function approximation.
- **Online Learning:** Facilitates online learning in dynamic environments.
- **Application in MDPs:** Well-suited for Markov Decision Processes.
- **Exploration Needed:** Requires a balance between exploration and exploitation.

**SARSA:**
- **On-Policy Method:** SARSA is an on-policy algorithm, updating action values based on the current policy.
- **Q-value Updates:** Uses the action taken in the next state to update the Q-value.
- **Exploration Handling:** Incorporates exploration strategies directly in the learning process.
- **Epsilon-Greedy:** Commonly used with ε-greedy exploration strategies.
- **State-Action Value Function:** Estimates the value of taking an action in a given state under a policy.
- **Immediate Feedback:** Updates Q-values based on immediate rewards and next actions.
- **Potential for Suboptimal Policies:** May lead to suboptimal policies if not balanced with exploration.
- **Training through Episodes:** Can be trained using episodic or continuing tasks.
- **Robustness:** More robust to changes in the environment compared to off-policy methods.
- **Convergence Guarantee:** Guarantees convergence under certain conditions.

**Q-Learning:**
- **Off-Policy Method:** Q-Learning is an off-policy algorithm that learns the optimal policy independently of the current policy.
- **Max Q-value Updates:** Updates Q-values based on the maximum estimated Q-value of the next state.
- **Exploration Strategies:** Typically uses ε-greedy or other exploration strategies.
- **Bellman Equation:** Directly applies the Bellman equation to update Q-values.
- **Experience Replay:** Can benefit from experience replay for better sample efficiency.
- **Convergence to Optimal Policy:** Guarantees convergence to the optimal policy with sufficient exploration.
- **Handling Large State Spaces:** Works well in environments with large or continuous state spaces.
- **Robustness:** More effective in stochastic environments compared to SARSA.
- **Learning Rate Control:** Requires careful tuning of learning rates for effective convergence.
- **Temporal-Difference Learning:** Combines TD learning with the exploration-exploitation trade-off.

---

### 4. Deep Q-networks (DQN, DDQN, Dueling DQN, Prioritized Experience Replay):

**Deep Q-Networks (DQN):**
- **Neural Network Approximation:** Uses neural networks to approximate the Q-value function.
- **Experience Replay:** Stores past experiences in a replay buffer for sampling and training.
- **Target Network:** Employs a separate target network to stabilize training.
- **Q-value Updates:** Updates Q-values using the Bellman equation and loss minimization.
- **Handling High Dimensions:** Effective in environments with high-dimensional state spaces.
- **Atari Games:** Successfully applied to Atari games, achieving human-level performance.
- **Convergence Improvement:** Improves convergence speed compared to traditional Q-learning.
- **Overestimation Bias:** Mitigates overestimation bias with target networks.
- **Experience Utilization:** Utilizes past experiences to improve sample efficiency.
- **Challenges in Training:** Faces challenges with hyperparameter tuning and stability.

**Double DQN (DDQN):**
- **Mitigating Overestimation:** Addresses the overestimation bias present in standard DQNs.
- **Separate Selection and Evaluation:** Separates action selection and Q-value evaluation using two networks.
- **Improved Action Value Estimates:** Uses the main network to select actions and the target network for value estimates.
- **Stable Learning:** Provides more stable learning compared to traditional DQN.
- **State Representation:** Efficiently handles large state representations.
- **Exploration Balance:** Maintains a balance between exploration and exploitation.
- **Experience Replay:** Benefits from experience replay for better training efficiency.
- **Performance Improvement:** Generally outperforms DQN in various tasks.
- **Robustness to Noise:** More robust to noise in Q-value estimates.
- **Complexity:** Slightly more complex due to the dual network architecture.

**Dueling DQN:**
- **Separate Value and

 Advantage Functions:** Separates the value function and advantage function in the network architecture.
- **Improved Q-value Representation:** Allows for more nuanced representation of state-action values.
- **Value Function Estimation:** Estimates the state value and action advantages independently.
- **Better Generalization:** Improves generalization in environments with similar states.
- **Reduced Variance:** Reduces variance in Q-value estimates for better stability.
- **Enhanced Learning:** Accelerates learning in sparse reward environments.
- **Action Selection Improvement:** Improves action selection in less significant states.
- **Robustness:** More robust to noisy rewards compared to traditional DQNs.
- **Network Complexity:** Slightly increases network complexity but offers performance benefits.
- **Effective Training:** Works effectively with experience replay.

**Prioritized Experience Replay:**
- **Experience Sampling:** Prioritizes important experiences for training based on the magnitude of the TD error.
- **Sample Efficiency:** Improves sample efficiency by focusing on more informative experiences.
- **Replay Buffer:** Maintains a replay buffer with prioritized experiences for training.
- **Efficient Learning:** Allows for faster learning by revisiting significant experiences more often.
- **Sampling Bias:** Introduces some bias in the sampling process, which needs to be managed.
- **Dynamic Priority Updates:** Continuously updates priorities as learning progresses.
- **Application in DQN:** Often combined with DQN architectures for enhanced performance.
- **Balancing Exploration:** Requires balancing between prioritized and uniform sampling.
- **Experience Quality:** Increases the quality of experiences used for training.
- **Complexity in Implementation:** Slightly increases implementation complexity due to priority management.

# **UNIT - 2**

Here are the points for each topic in bullet point form with bold keywords:

### 1. Policy Optimization: Introduction to Policy-Based Methods
- **Definition:** Policy-based methods directly parameterize and optimize the policy rather than the value function.
- **Advantages:** Effective in high-dimensional action spaces and continuous action domains.
- **Gradient Ascent:** Optimizes the expected return using gradient ascent on policy parameters.
- **Stochastic Policies:** Often employs stochastic policies for exploration purposes.
- **Actor-Critic Structure:** Can be combined with value function approximation using actor-critic methods.
- **Flexibility:** Allows for flexible policies, including deterministic and stochastic.
- **Sample Efficiency:** Generally less sample-efficient compared to value-based methods.
- **Convergence Properties:** Can converge to local optima depending on the optimization method used.
- **Application Areas:** Useful in robotics, game playing, and control tasks.
- **Challenges:** Requires careful tuning of hyperparameters and balancing exploration-exploitation.

---

### 2. Vanilla Policy Gradient
- **Policy Representation:** Represents policies as parameterized functions (e.g., neural networks).
- **Policy Gradient Theorem:** Uses the policy gradient theorem to derive gradients for policy optimization.
- **Direct Optimization:** Directly optimizes the expected return by following the gradient ascent direction.
- **Sample Efficiency:** Requires a large number of samples to achieve good performance.
- **Stochastic Nature:** Works well with stochastic policies, allowing exploration.
- **Variance Reduction:** High variance in gradient estimates can affect training stability.
- **Baseline Functions:** Use of baseline functions can help reduce variance without introducing bias.
- **Reward-to-Go:** Utilizes the reward-to-go for estimating returns, improving learning.
- **Simple Implementation:** Easier to implement compared to more advanced methods.
- **Applications:** Effective for simple tasks but struggles with complex environments.

---

### 3. REINFORCE Algorithm and Stochastic Policy Search
- **Monte Carlo Method:** REINFORCE uses Monte Carlo methods for estimating policy gradients.
- **Sample Returns:** Updates policy parameters based on complete episode returns.
- **High Variance:** Suffer from high variance in gradient estimates, making convergence slow.
- **Baseline Function:** Incorporates baseline functions to stabilize updates.
- **Policy Improvement:** Iteratively improves the policy based on collected returns.
- **Exploration Strategies:** Can use ε-greedy or softmax for exploration.
- **Generalization:** Effective in generalizing across similar states in episodic tasks.
- **Direct Policy Search:** Focuses on optimizing policies directly without relying on value functions.
- **Simple and Intuitive:** Conceptually simple and intuitive for policy optimization.
- **Limitations:** Inefficient for tasks requiring rapid learning due to variance.

---

### 4. Actor-Critic Methods (A2C, A3C)
- **Actor-Critic Framework:** Combines both policy-based (actor) and value-based (critic) methods.
- **Advantage Function:** Uses advantage functions to reduce variance in policy gradient estimates.
- **A2C (Advantage Actor-Critic):** Updates both actor and critic in parallel for improved stability.
- **A3C (Asynchronous Actor-Critic):** Utilizes multiple agents to collect experience asynchronously, enhancing exploration.
- **Sample Efficiency:** More sample-efficient than pure policy gradient methods.
- **Concurrent Learning:** Actors learn in parallel, improving training speed and robustness.
- **Deterministic Policy:** Can also use deterministic policies depending on the architecture.
- **Stability:** Actor-critic methods provide more stable training dynamics.
- **Flexible Framework:** Applicable to various environments and tasks.
- **Gradient Updates:** Alternates updates between actor and critic based on collected samples.

---

### 5. Advanced Policy Gradient (PPO, TRPO, DDPG)
- **Proximal Policy Optimization (PPO):** Balances exploration and exploitation while preventing large policy updates.
- **Clipped Objective:** Uses a clipped objective function to limit the change in policy.
- **Sample Efficiency:** More sample-efficient than traditional policy gradient methods.
- **Trust Region Policy Optimization (TRPO):** Enforces constraints on policy updates to ensure stability.
- **Natural Gradients:** Uses natural gradients for more efficient updates.
- **Deterministic Policy Gradient (DDPG):** Extends policy gradients to continuous action spaces using actor-critic architecture.
- **Experience Replay:** Utilizes experience replay to enhance learning efficiency in DDPG.
- **Exploration Noise:** Adds noise to actions for exploration in continuous environments.
- **Robust to Hyperparameters:** More robust to hyperparameter choices compared to earlier methods.
- **Real-World Applications:** Effective in complex tasks like robotic control and gaming.

---

### 6. Model-Based RL
- **Model Learning:** Involves learning a model of the environment to plan actions.
- **Planning Algorithms:** Uses planning algorithms (e.g., dynamic programming) to improve sample efficiency.
- **Transition Models:** Estimates transition dynamics to predict future states and rewards.
- **Value Function Approximation:** Can combine model learning with value function approximation methods.
- **Exploration Strategies:** Leverages models to optimize exploration strategies.
- **Computational Efficiency:** Reduces the need for extensive interaction with the environment.
- **Applications:** Useful in robotics, control, and decision-making tasks.
- **Hybrid Approaches:** Often combines model-free and model-based methods for better performance.
- **Challenges:** Requires accurate model estimation for effective planning.
- **Scalability:** Can be more scalable in complex environments with large state spaces.

---

### 7. Recent Advances and Applications: Meta-Learning, Multi-Agent Reinforcement Learning
- **Meta-Learning:** Focuses on learning to learn, enabling agents to adapt quickly to new tasks.
- **Transfer Learning:** Uses knowledge from previously learned tasks to improve learning efficiency on new tasks.
- **Multi-Agent RL:** Involves multiple agents learning and interacting in shared environments.
- **Cooperative and Competitive Learning:** Supports both cooperative and competitive scenarios among agents.
- **Scalability Challenges:** Faces challenges related to scalability and communication among agents.
- **Application Areas:** Applicable in game theory, autonomous vehicles, and resource allocation.
- **Policy Sharing:** Can share policies or value functions among agents to improve performance.
- **Communication Protocols:** Requires efficient communication protocols for coordination.
- **Complex Dynamics:** Models complex dynamics resulting from agent interactions.
- **Diversity in Strategies:** Explores diverse strategies for effective learning in multi-agent settings.

---

### 8. Partially Observable Markov Decision Process (POMDP)
- **State Uncertainty:** Deals with environments where the agent cannot fully observe the state.
- **Belief States:** Uses belief states to represent probability distributions over possible states.
- **Action-Observation Pairs:** Considers actions and observations to make decisions based on belief.
- **Complexity:** Increases complexity due to the need for maintaining and updating beliefs.
- **Planning Algorithms:** Requires specialized planning algorithms (e.g., point-based value iteration).
- **Applications:** Useful in robotics, surveillance, and any domain with partial observability.
- **Communication Needs:** May require agents to communicate to share information about the state.
- **Long-Term Planning:** Facilitates long-term planning in uncertain environments.
- **Model Learning:** Involves learning models for state transition and observation probabilities.
- **Challenges in Implementation:** Complexity in implementation and solving POMDPs.

---

### 9. Applying RL for Real-World Problems
- **Robotics:** Used for control tasks in robotic systems and automation.
- **Finance:** Applies RL to algorithmic trading and portfolio management.
- **Healthcare:** Assists in personalized medicine and treatment planning.
- **Game Development:** Enhances non-player character (NPC) behaviors and game dynamics.
- **Natural Language Processing:** Uses RL for dialogue systems and conversational agents.
- **Supply Chain Management:** Optimizes logistics, inventory, and resource allocation.
- **Autonomous Vehicles:** Improves decision-making and control in self-driving cars.
- **Energy Management:** Assists in optimizing energy consumption and distribution.
- **Smart Grids:** Implements RL for demand response and grid management.
- **Real-Time Decision Making:** Enables real-time decision-making in dynamic environments.
