### Aim

To run a simple Word Count application on a single-node Hadoop cluster to understand the MapReduce programming model and the workflow of a Hadoop application.

### Theory
**Hadoop** is a distributed computing framework that allows for the processing of large datasets across clusters of computers using simple programming models. The core components of Hadoop include:

- **Hadoop Common**: The common utilities that support the other Hadoop modules.
- **Hadoop Distributed File System (HDFS)**: A distributed file system that stores data across multiple machines.
- **Hadoop YARN**: The resource management layer of Hadoop that allocates system resources to various applications.
- **Hadoop MapReduce**: The processing layer of Hadoop, which provides a programming model for processing large datasets with a distributed algorithm on a cluster.

The **Word Count** example is a classic example that demonstrates how to use Hadoop MapReduce. It counts the occurrences of each word in a text file and outputs the results.

### Source Code

While using the Hadoop framework, you typically utilize existing libraries rather than writing the MapReduce code from scratch. However, below is the Java code for a basic Word Count program in Hadoop.

#### WordCount.java
```java
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {
    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String[] words = value.toString().split("\\s+");
            for (String w : words) {
                word.set(w);
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

### Steps to Run the Application

1. **Prepare Input Data**:
   Create a text file `input.txt` with the following content:
   ```
   Hello Hadoop
   Hello World
   Welcome to Hadoop
   ```

2. **Copy Input Data to HDFS**:
   ```bash
   hdfs dfs -mkdir /input
   hdfs dfs -put input.txt /input/
   ```

3. **Compile and Create a JAR File**:
   Use the following commands to compile the Java program and create a JAR file.
   ```bash
   javac -classpath `hadoop classpath` -d . WordCount.java
   jar cf wordcount.jar WordCount*.class
   ```

4. **Run the Word Count Program**:
   Execute the Word Count job:
   ```bash
   hadoop jar wordcount.jar WordCount /input/input.txt /output
   ```

5. **Check the Output**:
   List the output directory:
   ```bash
   hdfs dfs -ls /output
   ```

   Display the results:
   ```bash
   hdfs dfs -cat /output/part-r-00000
   ```

### Output
The output from the command will look like this:
```
Hello   2
Hadoop  2
Welcome 1
World   1
to      1
```

### Conclusion
The Word Count application demonstrates the basic functioning of Hadoop's MapReduce model. By implementing this simple application, we have seen how Hadoop processes data in a distributed manner, where tasks are divided into smaller subtasks that are processed in parallel. This experience lays the groundwork for more complex data processing tasks in big data analytics using Hadoop.
